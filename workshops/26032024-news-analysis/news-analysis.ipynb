{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AwONFeyVf3C"
      },
      "source": [
        "# Poverty Analysis in NL - workshop\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/Kickstartai-product/kai-workshops/blob/main/workshops/26032024-news-analysis/news-analysis.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7bkwCFc2mo_"
      },
      "source": [
        "## Setting up\n",
        "\n",
        "### Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wm8t2XaE2mo_"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!wget -O \"nos_data.csv.zip\" \"https://www.dropbox.com/scl/fi/5b0kkjih4xv9rnsfzbw63/nos_data_final_pruned.csv.zip?rlkey=ytgbvwj3238dnjd33wg7yu50y&dl=1\"\n",
        "!wget -O \"ipsos_polls.csv\" \"https://www.dropbox.com/scl/fi/gzsr1ocq63xez3n79x048/ipsos_polls.csv?rlkey=0hf2xqrdt6c8spw7czwi89mt9&dl=1\"\n",
        "!wget -O \"full_model_en.zip\" \"https://www.dropbox.com/scl/fi/1z4rhdupham561bar3ngx/full_model_en.zip?rlkey=k3c975ol1le94tunnevvlrzs5&dl=1\"\n",
        "!unzip -o nos_data.csv.zip\n",
        "!unzip -o full_model_en.zip\n",
        "!wget -O \"cbs_consumer_trust.csv\" \"https://www.dropbox.com/scl/fi/tre8belg4w6ujt81cimvq/cbs_consumer_trust.csv?rlkey=y4qugewaegpq0wfffz456ga8i&dl=1\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjWe4Mk92mpA"
      },
      "source": [
        "### Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_9s2Qq82mpA",
        "outputId": "7067a623-9ef0-49da-8202-e9d94afcfc87"
      },
      "outputs": [],
      "source": [
        "!pip3 install bertopic --no-deps\n",
        "!pip3 install pandas scikit-learn hdbscan umap-learn\n",
        "!pip3 install sentence-transformers --no-deps\n",
        "!pip3 install geopandas\n",
        "!pip3 install geoplot\n",
        "!pip3 install wordcloud\n",
        "!pip3 install plotly"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxMWzQer2mpA"
      },
      "source": [
        "### Imports and helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dtFINYXs2mpA"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import pandas as pd\n",
        "import bertopic as bt\n",
        "from bertopic.representation import TextGeneration\n",
        "from transformers import pipeline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import geopandas as gpd\n",
        "import geoplot as gplt\n",
        "import plotly.io as pio\n",
        "from wordcloud import WordCloud\n",
        "import re\n",
        "\n",
        "import random\n",
        "from umap import UMAP\n",
        "from hdbscan import HDBSCAN\n",
        "\n",
        "import scipy.sparse as sp\n",
        "\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
        "\n",
        "# set seed for reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0,\n",
        "                  metric='cosine', random_state=SEED)\n",
        "hdbscan_model = HDBSCAN(min_cluster_size=150, metric='euclidean',\n",
        "                        cluster_selection_method='eom', prediction_data=True)\n",
        "\n",
        "# TF-IDF Transformer helper class\n",
        "class CTFIDFVectorizer(TfidfTransformer):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(CTFIDFVectorizer, self).__init__(*args, **kwargs)\n",
        "\n",
        "    def fit(self, X: sp.csr_matrix, n_samples: int):\n",
        "        \"\"\"Learn the idf vector (global term weights) \"\"\"\n",
        "        _, n_features = X.shape\n",
        "        df = np.squeeze(np.asarray(X.sum(axis=0)))\n",
        "        idf = np.log(n_samples / df)\n",
        "        self._idf_diag = sp.diags(idf, offsets=0,\n",
        "                                  shape=(n_features, n_features),\n",
        "                                  format='csr',\n",
        "                                  dtype=np.float64)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X: sp.csr_matrix) -> sp.csr_matrix:\n",
        "        \"\"\"Transform a count-based matrix to c-TF-IDF \"\"\"\n",
        "        X = X * self._idf_diag\n",
        "        X = normalize(X, axis=1, norm='l1', copy=False)\n",
        "        return X\n",
        "\n",
        "# normalize helper function\n",
        "def normalize_series(series):\n",
        "    return (series - series.mean()) / series.std()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c95JcOmU22q1"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8fDLowII4oM"
      },
      "source": [
        "## Exercise 0: Loading the data and doing some exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PEatVVFbvzg"
      },
      "source": [
        "### Loading in the dataset\n",
        "\n",
        "This dataset has over a 150,000 news articles (rows), the followings columns are present:\n",
        "\n",
        "* `datetime`: time of posting\n",
        "* `title`: title of article\n",
        "* `category`: category of the news\n",
        "* `url`: URL of the article\n",
        "* `embedding`: OpenAI LLM embeddings of the article (not used in the notebook, but can be used later)\n",
        "* `location`, `lat`, `long`, `province`: location estimates of each article, obtained by Named Entity Recongition + Google Maps API\n",
        "* `translated_content`: content of the article, translated to English using Google Translate\n",
        "* `secret_cluster`: categorization of articles about poverty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cc__iS_blum",
        "outputId": "dfe99084-f86c-497e-ba4f-4d495c1d8c96"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('nos_data_final_pruned.csv', parse_dates=['datetime'])\n",
        "\n",
        "# cast embeddings to numpy array\n",
        "df['embedding'] = df['embedding'].apply(\n",
        "    lambda x: np.fromstring(x[1:-1], sep=',', dtype=np.float16))\n",
        "# sort by datetime\n",
        "df = df.sort_values(by='datetime')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "cwFcqYlFbyeW",
        "outputId": "29774a9c-ba10-453e-aa0f-2b8cac948c02"
      },
      "outputs": [],
      "source": [
        "# show first 5 rows of data frame to see what's in the data\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1pOwM6OJzaa"
      },
      "source": [
        "### Creating visualizations\n",
        "\n",
        "Let's create some basic visualizations about 3 aspects of the data:\n",
        "\n",
        "* The distribution of categories in the data\n",
        "* The distribution of articles over time\n",
        "* The spatial distribution of articles\n",
        "\n",
        "Are there results in line with your expectation? Why/Why not?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "ebKYsPuxb0zq",
        "outputId": "de06bee6-43e9-4f19-fd92-b8658a784d55"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(ncols=3, figsize=(20, 5))\n",
        "\n",
        "# Create pie plot of the categories to see the division of different categories in the data\n",
        "df['category'].value_counts().iloc[:5].plot(\n",
        "    kind='pie', title='Top 5 categories', xlabel='Category', ylabel='Count', ax=ax[0])\n",
        "\n",
        "# bar plot to show the number of articles per year\n",
        "df['datetime'].dt.year.value_counts().sort_index().plot(kind='bar', title='Articles per year', xlabel='Category', ylabel='Count', ax=ax[1]);\n",
        "# rotate x-axis labels for readability\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "# Geoplot to gain insights about locations that are more often in the news, we take a\n",
        "# map of the netherlands from stack.stanford.edu and plot the locations of the articles\n",
        "gdf = gpd.read_file('https://stacks.stanford.edu/file/druid:st293bj4601/data.zip')\n",
        "gdf = gdf[gdf['TYPE_1'] == 'Provincie']\n",
        "# drop all rows where ENGTYPE_1 is not 'Province'\n",
        "gdf.plot(ax=ax[2])\n",
        "xlims = ax[2].get_xlim()\n",
        "ylims = ax[2].get_ylim()\n",
        "ax[2].scatter(df['lng'], df['lat'], color='tab:orange', alpha=0.1, s=1)\n",
        "ax[2].set_xlim(*xlims)\n",
        "ax[2].set_ylim(*ylims)\n",
        "ax[2].set_title('Article Locations');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ju0d16GaeSWN"
      },
      "source": [
        "## Exercise 1: TF-IDF exploration\n",
        "\n",
        "We will be applying TF-IDF on a subset of our dataset, namely, on articles directly related to poverty. The goal is to find topics (words) in the articles about poverty that are special (very important) in that certain year. Those words are commonly that year, but less common in the others.\n",
        "\n",
        "We follow the following plan:\n",
        "\n",
        "1. Create a slice of the data that contains only poverty articles (we have classified this data already for you). We save this slice in a temporary data frame called `poverty_df`.\n",
        "\n",
        "2. We then divide our dataset into a separate cluster, each cluster representing articles from one calendar year. The first cluster, 2010, will be cluster 0, 2011 will be cluster 1, and so forth.\n",
        "\n",
        "3. We then fit the data on a `CountVectorizer`, a class from `sklearn`. This class performs Bag of Words, e.g., it turns our text into vectors that contain the frequency of all unique words.\n",
        "\n",
        "4. We then perform TF-IDF on each cluster, compared to all articles about poverty.\n",
        "\n",
        "5. We visualize the results in a word cloud for the year 2022.\n",
        "\n",
        "It can be initialized as follows:\n",
        "\n",
        "`count_vectorizer = CountVectorizer(stop_words=\"english\")`\n",
        "\n",
        "Where stop_words is set to English, so that the bag of words method skips stop words used in the English language.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "kWqY4my3dhJM",
        "outputId": "97aa92ea-9785-4d8c-87e5-f2bc5ce942d8"
      },
      "outputs": [],
      "source": [
        "# create the poverty dataframe, ignoring NaN values in the secret_cluster column\n",
        "poverty_df = df.dropna(subset=['secret_cluster']).copy()\n",
        "# select all rows where the secret_cluster is 1, where 1 indicates articles related to poverty\n",
        "poverty_df = poverty_df[poverty_df['secret_cluster'] == 1]\n",
        "# get the year for each article, save it in a new column called 'year'\n",
        "poverty_df['year'] = poverty_df['datetime'].dt.year\n",
        "# Group by year\n",
        "poverty_df = poverty_df[['translated_content', 'year']]\n",
        "# show the dataframe\n",
        "poverty_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2y51FQG12mpB"
      },
      "source": [
        "Now, we group all documents of each year together. This makes each year one big 'article', such that we can easily count the word occurance for each year.\n",
        "\n",
        "The output you get should look like this:\n",
        "\n",
        "|    |   year | translated_content           |\n",
        "|---:|-------:|:-----------------------------|\n",
        "|  0 |   2010 | Prime Minister Balkenende... |\n",
        "|  1 |   2011 | By editor Sander Warmerda... |\n",
        "|  2 |   2012 | One in five Greeks lives ... |\n",
        "|  3 |   2013 | Hundreds of church buildi... |\n",
        "| ... | ... | ... |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "EYsbpIdP2mpB",
        "outputId": "545ac73f-b680-474b-b1ef-ffbd1137b07a"
      },
      "outputs": [],
      "source": [
        "# group by year and concatenate all articles in that year\n",
        "articles_per_year = poverty_df.groupby(['year'], as_index=False).agg({'translated_content': ' '.join})\n",
        "# show the dataframe (first 5 rows)\n",
        "articles_per_year.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mh_tNOGT2mpB"
      },
      "source": [
        "Now, we perform Bag of Words using sklearns CountVectorizer. We ignore English stop words, in the end, we obtain:\n",
        "\n",
        "* `count`: An array with counts for each unique word and all the articles per year. The array has shape `(N_years, N_unique_words)`\n",
        "* `word`: An array with all unique words. This array has a shape `(N_unique_words)`, there are over 17,000 unique words found!\n",
        "\n",
        "In the end, we print an examle of 10 identified words:\n",
        "\n",
        "`array(['judges', 'judgment', 'judgments', 'judicial', 'judiciary',\n",
        "       'judikje', 'judith', 'juffermans', 'juice', 'juliana'],\n",
        "      dtype=object)`\n",
        "\n",
        "Can you spot a weakness of Bag of Words based on this output?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ys-tjlIJ2mpB",
        "outputId": "2a116e62-413f-490c-8779-8e6e77b0ddc2"
      },
      "outputs": [],
      "source": [
        "# create a bag of words class using CountVectorizer\n",
        "count_vectorizer = CountVectorizer(stop_words=\"english\")\n",
        "# get the counts of each word in the corpus, for each class\n",
        "count = count_vectorizer.fit_transform(articles_per_year.translated_content)\n",
        "# get the unique words in the corpus\n",
        "words = count_vectorizer.get_feature_names_out()\n",
        "\n",
        "# example of the first 10 words\n",
        "words[len(words)//2: len(words)//2 + 10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNmm2tQ62mpC"
      },
      "source": [
        "Now we perform TF-IDF, this class calculates the TF-IDF values for each article cluster, for each term, as such, it has the same shape as `count`.\n",
        "\n",
        "This results in an array with a score for every year and every unique word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j050U-qS2mpC",
        "outputId": "19fadeba-46a8-42c4-adbc-915dff0ad384"
      },
      "outputs": [],
      "source": [
        "# Create a TF-IDF class, using the helper class defined in the beginning of this notebook\n",
        "ctfidf = CTFIDFVectorizer().fit_transform(count, n_samples=len(poverty_df)).toarray()\n",
        "\n",
        "ctfidf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fB038E4X2mpC"
      },
      "source": [
        "We select a year using `selected_year`. You can choose any year in the range 2010-2023"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2twuxo1QO8F1"
      },
      "outputs": [],
      "source": [
        "# Select a year. Note you can change the year as long as its in the range 2010-2023\n",
        "selected_year =  2022"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRVxYUNtPXNC"
      },
      "source": [
        "Show the top 25 terms and their TF-IDF values. Do you think it makes sense?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "id": "eOHrogrM2mpC",
        "outputId": "b7faa115-5dac-4cfe-ff8e-92edf18d37bc"
      },
      "outputs": [],
      "source": [
        "# get values for the selected year, the index starts at 2010\n",
        "ctfidf_values = ctfidf[selected_year - 2010]\n",
        "# get the indices of the top 25 words\n",
        "top_25_indices = ctfidf_values.argsort()[-25:][::-1]\n",
        "# get the 25 words with the highest TF-IDF values\n",
        "top_25_words = {words[index]: ctfidf_values[index] for index in top_25_indices}\n",
        "# get the tfidf values for the top 25 words\n",
        "top_25_values = [ctfidf_values[index] for index in top_25_indices]\n",
        "# create a dataframe with the top 25 words and their tfidf values\n",
        "top_25_df = pd.DataFrame({'word': list(top_25_words.keys()), 'tfidf': list(top_25_words.values())})\n",
        "\n",
        "top_25_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEUfqMyk2mpC"
      },
      "source": [
        "Now, let's make a word cloud of the terms!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 836
        },
        "id": "p0l41QOT2mpC",
        "outputId": "2ebfd23b-735e-4933-c203-2fa01fe208d5"
      },
      "outputs": [],
      "source": [
        "# Convert the list of keywords into a single string with spaces\n",
        "text = \" \".join(top_25_df['word'])\n",
        "\n",
        "# Create a WordCloud object\n",
        "wordcloud = WordCloud(width = 800, height = 800,\n",
        "                background_color ='white',\n",
        "                min_font_size = 10).generate(text)\n",
        "\n",
        "# Display the generated image:\n",
        "plt.figure(figsize = (8, 8), facecolor = None)\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout(pad = 0)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiGdYe9N2mpC"
      },
      "source": [
        "## Exercise 2: BERTopic\n",
        "\n",
        "In this exercise we will visualize the embedding space used by BERTopic. We created the embeddings already.\n",
        "\n",
        "1. We will use BERTopic to visualize the embeddings in a UMAP, clustering similar topics. Note, the code is commented out, because it takes a long time to fit the model. However, you can run this code later.\n",
        "2. Next we will look at the topics identified and what is in their representation.\n",
        "3. You can visualize the embedding space. As this takes a long time we will provide the HTML, so we can check it out. Later you can run this part of the code as well.  \n",
        "\n",
        "\n",
        "The results of the visualisation can be seen by clicking on this [link](https://www.dropbox.com/scl/fi/v3pvl10q6axzwyyv4jnt1/visualize_documents_top25.html?rlkey=kf9o4sgrhlogsz2caw7ki8fft&dl=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYkdIqTy2mpC"
      },
      "source": [
        "This is how you would fit the (clustering) model, depending on your CPU/GPU setup.\n",
        "For now, we'll skip this part and load the fitted model in."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lk2zwbLX2mpC"
      },
      "outputs": [],
      "source": [
        "\"\"\" Fit the BERTopic model \"\"\"\n",
        "# documents = df['translated_content'].tolist()\n",
        "# embeddings = np.array(df['embedding'].tolist())\n",
        "\n",
        "# vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
        "# bertopic_model = bt.BERTopic(umap_model=umap_model, hdbscan_model=hdbscan_model, verbose=True, vectorizer_model=vectorizer_model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHDp0HyEIftY"
      },
      "source": [
        "Look at the topics and their representations.\n",
        "\n",
        "Note: `-1` is a remainder topic, all articles that couldn't be fitted are stored here. For this reason we will ignore `Topic` `-1`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 871
        },
        "id": "q-bxZ9dP2mpC",
        "outputId": "3c37d6cb-8be7-480a-f924-9cb9ef1f1adb"
      },
      "outputs": [],
      "source": [
        "bertopic_model = bt.BERTopic.load(\"full_model_en\")\n",
        "df['topic'] = bertopic_model.topics_\n",
        "bertopic_model.get_topic_info().head(25).iloc[:, :-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "x4o6o9Cx7ns2",
        "outputId": "a1c73c6a-5d16-4c02-965b-c271d8f4d416"
      },
      "outputs": [],
      "source": [
        "bertopic_model.get_topic_info().head(25).iloc[:, :-1][\"Name\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UCmlB_o2mpC"
      },
      "source": [
        "This is how you could generate a latent space visualization, again, we'll skip this for now as it is quite CPU intensive, and provide you the [html](https://www.dropbox.com/scl/fi/v3pvl10q6axzwyyv4jnt1/visualize_documents_top25.html?rlkey=kf9o4sgrhlogsz2caw7ki8fft&dl=1) file directly.\n",
        "If you have already downloaded the file, no need to click on this link again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gdd56NAW2mpC"
      },
      "outputs": [],
      "source": [
        "\"\"\"Reduce dimensionality of embeddings, this step is optional but much faster to perform iteratively:\"\"\"\n",
        "# reduced_embeddings = UMAP(n_neighbors=10, n_components=2, min_dist=0.0, metric='cosine', random_state=SEED).fit_transform(embeddings)\n",
        "# fig = bertopic_model.visualize_documents(documents, reduced_embeddings=reduced_embeddings, topics=list(range(1, 26)))\n",
        "# pio.write_html(fig, file='visualize_documents.html', auto_open=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cq_aGe_U2mpC"
      },
      "source": [
        "\n",
        "## Exercise 3: Spatial distribution of topics\n",
        "In this exercise we investigate how news topics are distributed geographically.\n",
        "We do this by:\n",
        "1. Selecting a specific topic. You can select your own by running `bertoptic_model.get_topic_info()` or choose some of the topics we suggest below.\n",
        "2. Create a plot where news articles are plotted on their approximate locations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KCUCJIxBUaE7"
      },
      "outputs": [],
      "source": [
        "# Take a selected topic, to see all topics, run bertopic_model.get_topic_info()\n",
        "# Suggestions:\n",
        "selected_topic = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        },
        "id": "ZNJVLrkV2mpD",
        "outputId": "d4fdde3a-ea2b-4242-b45e-a41fbba5ce69"
      },
      "outputs": [],
      "source": [
        "# take a selected topic, to see all topics, run bertopic_model.get_topic_info()\n",
        "selected_topic = 1\n",
        "df_topic = df[df['topic'] == selected_topic].dropna(subset=['lng', 'lat'])\n",
        "\n",
        "# plot the dutch geodata\n",
        "# Load shapefiles of NL\n",
        "gdf = gpd.read_file('https://stacks.stanford.edu/file/druid:st293bj4601/data.zip')\n",
        "\n",
        "# take a selected topic, to see all topics, run bertopic_model.get_topic_info()\n",
        "selected_topic = 11\n",
        "df_topic = df[df['topic'] == selected_topic].dropna(subset=['lng', 'lat'])\n",
        "\n",
        "# make geo dataframe, has cols lat and lng\n",
        "gdf_temp = gpd.GeoDataFrame(df_topic, crs=\"EPSG:4326\", geometry=gpd.points_from_xy(df_topic.lng, df_topic.lat, crs=\"EPSG:4326\"))\n",
        "\n",
        "# limit to articles with geo location in NL\n",
        "points_within_gdf = gpd.sjoin(gdf_temp, gdf, how='inner', op='within')\n",
        "\n",
        "# plot\n",
        "ax = gplt.polyplot(gdf, projection=gplt.crs.AlbersEqualArea(), zorder=1)\n",
        "gplt.kdeplot(\n",
        "      points_within_gdf, cmap='Reds', shade=True, ax=ax,\n",
        "      thresh=0.1,\n",
        "      alpha=.9\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4-kH_rQVvG6"
      },
      "source": [
        "Why do you think a lot of these plots look similar? Do you have an idea of how to fix this?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8E5ZdRo32mpD"
      },
      "source": [
        "## Exercise 4: Analysis using topics developments\n",
        "\n",
        "In this exercise we will 1) explore how the frequencies of news mentions within a topic change over time, and 2) see if we can find correlections with the perception of economics.\n",
        "\n",
        "To do this, we take the following steps:\n",
        "\n",
        "\n",
        "1.   Get the topic counts for every month in the years 2010 to 2023. The topics are the topics found using BERTopic clustering (UMAP).\n",
        "2.   Normalize the counts\n",
        "3. Select a topic and plot the counts for that topic over time (months in the years 2010 to 20230).\n",
        "4. Use economic perception data from the CBS and select one indicator to plot it over time.\n",
        "5. Combine economic perception data and a selected topic from our news data and see if there are any correlations vissible.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hvGfCUK-Vt1"
      },
      "source": [
        "First we create a dataframe where for every month in the years 2010 to 2023 the topic mentions are counted.\n",
        "In other words, we count how many times a topic e.g., `4_students_education_school_schools` is mentioned in a specific month."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 588
        },
        "id": "7WTusbda2mpD",
        "outputId": "5488938d-77c8-47c6-838e-46c9c597535c"
      },
      "outputs": [],
      "source": [
        "# get calendar months\n",
        "df['month'] = df['datetime'].dt.to_period('M').dt.to_timestamp()\n",
        "# group by month and topic and count the number of articles\n",
        "monthly_topic_counts = df.groupby(['month', 'topic']).size().unstack().fillna(0).iloc[:, 1:]\n",
        "# set the BERTopic topic names as columns\n",
        "monthly_topic_counts.columns = bertopic_model.get_topic_info()['Name'].iloc[1:]\n",
        "\n",
        "# visualize a part of the newly created data frame to see what's in it.\n",
        "monthly_topic_counts.iloc[:10, :5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oklM4R30_HqU"
      },
      "source": [
        "Normalize the topic counts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 588
        },
        "id": "560MusBi2mpD",
        "outputId": "ec93ec1c-dd66-45d8-d7d4-d2f7a1bb6029"
      },
      "outputs": [],
      "source": [
        "# normalize the counts by dividing by the total number of articles in that month\n",
        "normalized_monthly_topic_counts = monthly_topic_counts.div(monthly_topic_counts.sum(axis=1), axis=0)\n",
        "\n",
        "# visualize part of the data frame, to see the normalized counts\n",
        "normalized_monthly_topic_counts.iloc[:10, :5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKYlLllm_ht0"
      },
      "source": [
        "Choose a topic to plot.\n",
        "We have made a selection of interesting topics you can choose from:\n",
        "\n",
        "But, you can also choose any topic yourself as long as it was found by the BERTopic clustering we did before `bertopic_model.get_topic_info()['Name']`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-T4ChIWALdg"
      },
      "outputs": [],
      "source": [
        "# Set your own selected topic to visualize\n",
        "selected_topic = '0_patients_care_healthcare_hospital'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9gu7aKWF8m-"
      },
      "source": [
        "Plot `selected_topic` over time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "2soTHK7d2mpD",
        "outputId": "30c6ea17-2c03-4f54-98d9-9e0eaa7e5152"
      },
      "outputs": [],
      "source": [
        "normalized_monthly_topic_counts[selected_topic].plot(xlabel='Month', ylabel='Count', title='Monthly article count for topic: ' + selected_topic);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgpfWis5Gk88"
      },
      "source": [
        "Next we will look at the economic perception data from the CBS. We will first load the data into a data frame. Then we will set the index of the data to months and look at the variable `Koopbereidheid` by plotting it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "m_hRatwg2mpK",
        "outputId": "3aa63dc2-17c2-4837-b28d-4c0bcce5858d"
      },
      "outputs": [],
      "source": [
        "# load CBS data in data frame\n",
        "df_cbs = pd.read_csv('cbs_consumer_trust.csv', sep=';')\n",
        "# set index to month\n",
        "df_cbs['month'] = pd.to_datetime(df_cbs['Perioden'], format='%YMM%m')\n",
        "df_cbs.set_index('month', inplace=True)\n",
        "# plot the Koopbereidheid variable\n",
        "df_cbs['Koopbereidheid'].plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGcXRNlSG-gz"
      },
      "source": [
        "Now we will combine the plots from our news data with the CBS plot, to see if we can (visually) identify any correlations.\n",
        "\n",
        "As an example we have used the following columns from the data set:\n",
        "- From the news data we use the topic `16_housing_mortgage_homes_rental`\n",
        "- From the CBS data we use the feature `EconomischeSituatieKomende12Maanden`\n",
        "\n",
        "However, you can use any topics and features you want to explore if you can find any other interesting correlations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WE1ty63qHmpW"
      },
      "outputs": [],
      "source": [
        "# Select the topic(s) from the news data you want to visualize\n",
        "selected_topics = ['16_housing_mortgage_homes_rental']\n",
        "\n",
        "# Select a CBS feature you want to plot\n",
        "cbs_feature = 'EconomischeSituatieKomende12Maanden'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "U1He0sl82mpK",
        "outputId": "d38ec718-ba6b-441d-a330-8e9b5ceb90ba"
      },
      "outputs": [],
      "source": [
        "# create a combined data frame of the selected news topic(s) and the cbs_feature\n",
        "plot_df = normalized_monthly_topic_counts[selected_topics].join(-df_cbs[cbs_feature])\n",
        "# apply normalization to the columns\n",
        "plot_df = plot_df.apply(normalize_series)\n",
        "# apply 3 month rolling average\n",
        "plot_df = plot_df.rolling(6).mean()\n",
        "\n",
        "# plot the data frame\n",
        "plot_df.plot(xlabel='Month', ylabel='Normalized count', title='Monthly article count for topic: ' + selected_topic + ' and consumer trust');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y422ZniN2mpK"
      },
      "outputs": [],
      "source": [
        "# TODO: add some possible interesting things participants can do\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFR1OJOusWKJ"
      },
      "source": [
        "## Survey\n",
        "\n",
        "Please let us know what you think about this workshop by filling in our [survey](https://workshop-march26.kickstartai-events.org/page/1498843).\n",
        "\n",
        "Thank you!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZCVH35a8MEl"
      },
      "source": [
        "### Backup\n",
        "\n",
        "Loading data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KHmFSZw8Qdq"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# nos_data.csv.zip\n",
        "!gdown 1NZf16cNgIqKhQvVzqW65Nivds_tWPxuE\n",
        "\n",
        "# ipsos_polls.csv\n",
        "!gdown 1qwBMbZvo9R6QDKhiJIyOa8f28h5Hfydu\n",
        "\n",
        "# full_model_en.zip\n",
        "!gdown 1Pze19r-k0wvzN6ZemNwCWeYz3A1nRPgG\n",
        "\n",
        "# cbs_consumer_trust.csv\n",
        "!gdown 1oqMIHMNNLVjyrOSDwbOXAZw4sVCkgIqZ\n",
        "\n",
        "!unzip -o nos_data.csv.zip\n",
        "!unzip -o full_model_en.zip"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
